{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict all cause 30-day hospital readmission risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand the relationship between different tables and the data in those tables. This is important to identify the information which is relevant to the prediction. The tool that you used to generate the data created different csv files which you will upload to S3 bucket. Based on the generated data, you can see the below relationship between different tables within your data set. If you are using your own data for this notebook, it will help to create some visualization of the data to better understand the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Visualizing Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"EHR.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved in this machine learning project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understanding of your data \n",
    "2. Storing and converting your data into parquet for optimized performance and storage\n",
    "3. Feature selection and feature engineering using Spark\n",
    "4. Data pre-processing - StringIndexer and OneHotEncoding to convert categorical variables into required training data\n",
    "5. Train Spark ML model for data pre-processing and serialize using MLeap library to be used during inference pipeline\n",
    "6. Convert the data set into XGBoost supported format i.e. CSV from Spark Data Frame\n",
    "7. Split the data set into training and validation for model training and validation\n",
    "8. Train XGBoost Model using SageMaker XGBoost algorithm and validate model prediction using validation data set\n",
    "9. Tune the trained model using Hyperparameter tuning jobs for required HPO parameters\n",
    "10. Get the best tuned model and create inference pipeline which includes Spark ML model and XGBoost Model\n",
    "11. Create the end point configuration to deploy the inference pipeline\n",
    "12. Deploy the inference pipeline for real time prediction\n",
    "13. Invoke real time prediction API for a request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to **update** **S3 Bucket** and **KMS Key Id** with the values for your environment. This notebook requires certain resources to be created. Cloud Formation template has been provided to create the required resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore \n",
    "import time\n",
    "\n",
    "bucket = '' # Update this to the bucket that was created in your lab account as part of this enviroment.\n",
    "sse_kms_id = '' ## Get this value from Cloud Formation template\n",
    "glue_crawler_db = 'ehr-db-readmission' ## Glue Database created by Glue crawler (contains raw data)\n",
    "s3 = boto3.resource('s3')\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing using Apache Spark in AWS Glue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Glue Scripts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download Dependencies\n",
    "wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
    "wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Uploading Glue scripts and dependencies to S3\n",
    "from sagemaker import Session as Sess\n",
    "\n",
    "# SageMaker session\n",
    "sess = Sess()\n",
    "\n",
    "result = sess.upload_data(path='../glue_scripts/convert_to_parquet', bucket=bucket, key_prefix='scripts', extra_args={\"ServerSideEncryption\": \"aws:kms\",'SSEKMSKeyId':sse_kms_id })\n",
    "print(result)\n",
    "result = sess.upload_data(path='../glue_scripts/produce_training_data', bucket=bucket, key_prefix='scripts', extra_args={\"ServerSideEncryption\": \"aws:kms\",'SSEKMSKeyId':sse_kms_id })\n",
    "print(result)\n",
    "result = sess.upload_data(path='python.zip', bucket=bucket, key_prefix='scripts', extra_args={\"ServerSideEncryption\": \"aws:kms\",'SSEKMSKeyId':sse_kms_id })\n",
    "print(result)\n",
    "result = sess.upload_data(path='mleap_spark_assembly.jar', bucket=bucket, key_prefix='scripts', extra_args={\"ServerSideEncryption\": \"aws:kms\",'SSEKMSKeyId':sse_kms_id })\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "#### SKIP THIS STEP IF YOU GENERATED YOUR OWN DATA SET. MAKE SURE TO UPDATE SCRIPTS ACCORDINGLY AS PER YOUR  DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download raw data\n",
    "wget https://hospital-readmission-blog.s3-us-west-2.amazonaws.com/synthetic_5000_population.zip\n",
    "unzip -d raw-data synthetic_5000_population.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$bucket\" \"$sse_kms_id\"\n",
    "\n",
    "# Uploading raw data to S3\n",
    "export bucket=$1\n",
    "export sse_kms_id=$2\n",
    "\n",
    "YY=$(date +%Y)\n",
    "MM=$(date +%m)\n",
    "DD=$(date +%d)\n",
    "\n",
    "aws s3 cp raw-data/allergies.csv s3://$bucket/raw-data/allergies/$YY/$MM/$DD/allergies.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/imaging_studies.csv s3://$bucket/raw-data/imaging_studies/$YY/$MM/$DD/imaging_studies.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/payer_transitions.csv s3://$bucket/raw-data/payer_transitions/$YY/$MM/$DD/payer_transitions.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/patients.csv s3://$bucket/raw-data/patients/$YY/$MM/$DD/patients.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/encounters.csv s3://$bucket/raw-data/encounters/$YY/$MM/$DD/encounters.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/conditions.csv s3://$bucket/raw-data/conditions/$YY/$MM/$DD/conditions.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/medications.csv s3://$bucket/raw-data/medications/$YY/$MM/$DD/medications.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/careplans.csv s3://$bucket/raw-data/careplans/$YY/$MM/$DD/careplans.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/observations.csv s3://$bucket/raw-data/observations/$YY/$MM/$DD/observations.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/procedures.csv s3://$bucket/raw-data/procedures/$YY/$MM/$DD/procedures.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/immunizations.csv s3://$bucket/raw-data/immunizations/$YY/$MM/$DD/immunizations.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/organizations.csv s3://$bucket/raw-data/organizations/$YY/$MM/$DD/organizations.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/providers.csv s3://$bucket/raw-data/providers/$YY/$MM/$DD/providers.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n",
    "aws s3 cp raw-data/payers.csv s3://$bucket/raw-data/payers/$YY/$MM/$DD/payers.csv --sse aws:kms --sse-kms-key-id $sse_kms_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your S3 Bucket is now ready with raw data and required scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check for uploaded raw data on S3 bucket\n",
    "!aws s3 ls 's3://'$bucket'/raw-data/' --recursive --page-size=1 --human-readable --summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at Patients Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print (\"Patients Data\")\n",
    "df = pd.read_csv('raw-data/patients.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at Encounters Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print (\"Encounters Data\")\n",
    "df = pd.read_csv('raw-data/encounters.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run AWS Glue Preprocessing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll be creating Glue client via Boto3 so that we can invoke the start_job_run API of Glue. This API creates an immutable run/execution corresponding to the job definition created above. We will require the job_run_id for the particular job execution to check for status. We'll pass the data and model locations as part of the job execution parameters.\n",
    "\n",
    "Finally, we will check for the job status to see if it has succeeded, failed or stopped. Once the job is succeeded, we have the transformed data into S3 in required format. If the job fails, you can go to AWS Glue console, click on Jobs tab on the left, and from the page, click on this particular job and you will be able to find the CloudWatch logs (the link under Logs) link for these jobs which can help you to see what exactly went wrong in the job execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start AWS Glue crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can now login to [AWS console](https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=crawlers) to run AWS Glue crawler, look for the crawler named **ehr-crawler-readmission** (default name provided in CloudFormation) and run the crawler. Once the crawler is successfully run i.e. the attribute **Tables Added** will be updated to the number of tables discovered by the crawler. Below screenshots can guide you through the process.\n",
    "\n",
    "- Click on Databases in AWS Glue console and look for database named **ehr-db-readmission**(default name provided in CloudFormation) and click on it. You can then click on the link `Tables in ehr-db-readmission` to check the available tables and associated properties. \n",
    "\n",
    "![Run Crawler](../images/6.png)\n",
    "\n",
    "![Run Crawler](../images/7.png)\n",
    "\n",
    "![Glue Database](../images/8.png)\n",
    "\n",
    "![Glue Database](../images/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start CSV to Parquet conversion Glue Job and Wait for Job to Succeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Glue Job is setup to use Spark 2.4 and Python 3.0. The job requires a python script which is uploaded to the S3 bucket and provided to the job while creating this job using Cloud Formation template. You can generate these scripts in Glue using the console so that you don't have to write the script from scratch and can make modifications to the generated script as per your use case. In this case, we generated the script to read the data from Glue crawler database and then selecting only the required columns based on domain knowledge for pre-processing and model training. We will drop the null values and update the data types to be supported by our machine learning algorithm. Finally, the data set is saved to S3 bucket in parquet with partition keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and run AWS Glue Preprocessing Job to convert CSV to Parquet Format\n",
    "\n",
    "# Define the Job in AWS Glue\n",
    "glue = boto3.client('glue')\n",
    "\n",
    "try:\n",
    "    glue.get_job(JobName='glue-etl-convert-to-parquet')\n",
    "    print(\"Job already exists, continuing...\")\n",
    "except glue.exceptions.EntityNotFoundException:\n",
    "    print('{}\\n'.format(\"Job Not Found, Check the output of Cloud Formation template\"))\n",
    "\n",
    "# Run the job in AWS Glue\n",
    "try:\n",
    "    job_name='glue-etl-convert-to-parquet'\n",
    "    response = glue.start_job_run(JobName=job_name,\n",
    "                                  Arguments={\n",
    "                                            '--s3_bucket' : bucket,\n",
    "                                            '--glue_crawler_db' : glue_crawler_db ##This value is from cloud formation template\n",
    "                                    })\n",
    "    job_run_id = response['JobRunId']\n",
    "    print('{}\\n'.format(response))\n",
    "except glue.exceptions.ConcurrentRunsExceededException:\n",
    "    print(\"Job run already in progress, continuing...\")\n",
    "\n",
    "\n",
    "job_url = \"https://console.aws.amazon.com/glue/home?region=\"+region+\"#etl:tab=jobs\"\n",
    "print (\"You can go to AWS Glue Console to check status for glue-etl-convert-to-parquet as shown in below screenshots: \"+job_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Glue Database](../images/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Glue Job to Produce Training Data\n",
    "#### Note - This job runs for approximately 30 min depending on the data set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and run AWS Glue Preprocessing Job\n",
    "\n",
    "# Define the Job in AWS Glue\n",
    "glue = boto3.client('glue')\n",
    "\n",
    "try:\n",
    "    glue.get_job(JobName='glue-etl-produce-traing-data')\n",
    "    print(\"Job already exists, continuing...\")\n",
    "except glue.exceptions.EntityNotFoundException:\n",
    "    print('{}\\n'.format(\"Job Not Found, Check the output of Cloud Formation template\"))\n",
    "\n",
    "# Run the job in AWS Glue\n",
    "try:\n",
    "    job_name='glue-etl-produce-traing-data'\n",
    "    response = glue.start_job_run(JobName=job_name,\n",
    "                                  Arguments={\n",
    "                                            '--sse_kms_id': sse_kms_id,\n",
    "                                            '--s3_bucket' : bucket\n",
    "                                    })\n",
    "    job_run_id = response['JobRunId']\n",
    "    print('{}\\n'.format(response))\n",
    "except glue.exceptions.ConcurrentRunsExceededException:\n",
    "    print(\"Job run already in progress, continuing...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Glue Job setup uses **Spark 2.2 and Python 2.0**. The job requires a python script which is uploaded to the S3 bucket from the code repository. In this case, we are using Spark 2.2 instead of latest supported Spark version i.e. Spark 2.4 since **MLeap** serialization libraries provided for serializing Spark ML model for data pre-processing currently does not support Spark 2.4. You can check more details about this on https://github.com/aws/sagemaker-sparkml-serving-container. In the provided script, all S3 partitions are read but you have the flexibility to filter partitions to read data based on required partition key. In this solution, partitions are based on the date but you can define your own *partition strategy*. As per the understanding of your data and domain knowledge, multiple tables are joined and unncessary columns are dropped to perform feature selection. Since this notebook uses **XGBoost** which is a Supervised learning model, you need to provide **Label data** information in the training data set. The script calculates Label data i.e. **30-day readmission** by sorting all the patient encounters by timestamp for a specific patient_id and then taking **a difference of encounter_stop from previous encounter and encounter_start from current encounter** which provides the number of hours from the last encounter and can be used to identify if the encounter was within last 30 days or not. For Feature engineering, Imputation technique is used to fill some of the missing values in the training data. The script also converts **birth_date** into **age** for feature engineering. Once all this is done, it generates feature vector by leveraging **Spark ML OneHotEncoding** and then serializes the model using MLeap serialization library. Since XGBoost algorithm supports CSV format for training data, you need to convert Spark Data Frame into CSV files and save to S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check AWS Glue Job Status and Wait for it to Succeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_url = \"https://console.aws.amazon.com/glue/home?region=\"+region+\"#etl:tab=jobs\"\n",
    "print (\"You can go to AWS Glue Console to check status for glue-etl-produce-traing-data: \"+job_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore AWS Glue Jobs\n",
    "![Glue Jobs](../images/14.png)\n",
    "![Glue Jobs](../images/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Validate Spark ML model for data pre-processing\n",
    "Once the Open Jupyter notebook i.e. **Sparkml-model-test.ipynb** to understand and validate the prediction generated by Spark ML model for data pre-processing. Once done, come back to this notebook.\n",
    "![Sparkml-model-test.ipynb](../images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Amazon SageMaker XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have our data preprocessed in a format that XGBoost recognizes, you can run a simple training job to train a binary classifier model on our data. You can run this entire process in this Jupyter notebook. Run the following cell, labeled **Run Amazon SageMaker XGBoost Training Job**. This runs **XGBoost algorithm** training job in Amazon SageMaker, and monitors the progress of the job. Once the job is ‘Completed’, you can move on to the next cell.\n",
    "\n",
    "After a few minutes, the job should complete successfully, and output model artifacts saved to the specified S3 location. Once this is done, the model is deployed to an inference pipeline that consists of pre-processing, inference and post-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Amazon SageMaker XGBoost Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will use SageMaker XGBoost algorithm to train model on this dataset. You already have the S3 location\n",
    "where the preprocessed training data was uploaded as result of the Glue pre-processing job. You need to update **train_prefix** and **validation_prefix** with S3 prefix location of training and validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get S3 Location of Training Data Set\n",
    "!aws s3 ls 's3://'$bucket'/train-data/' --recursive --page-size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get S3 Location of Validation Data Set\n",
    "!aws s3 ls 's3://'$bucket'/validation-data/' --recursive --page-size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get S3 Location of Validation Data Set\n",
    "!aws s3 ls 's3://'$bucket'/test-data/' --recursive --page-size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Update training and validation data set S3 prefix \n",
    "train_prefix = \"train-data/2020/4/9\" ## Update S3 Prefix from above output\n",
    "validation_prefix = \"validation-data/2020/4/9\" ## Update S3 Prefix from above output\n",
    "test_data_prefix = 'test-data/2020/4/9' ## Update S3 Prefix from above output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from sagemaker import Session as Sess\n",
    "\n",
    "# SageMaker session\n",
    "sess = Sess()\n",
    "\n",
    "# Boto3 session\n",
    "session = boto3.session.Session()\n",
    "role = get_execution_role()\n",
    "region = session.region_name\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'xgboost', repo_version=\"0.90-1\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Amazon SageMaker XGBoost Training Job\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Get XGBoost container image for current region\n",
    "training_image = get_image_uri(region, 'xgboost', repo_version=\"0.90-1\")\n",
    "\n",
    "# Create a unique training job name\n",
    "training_job_name = 'xgboost-readmission-'+''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(8))\n",
    "\n",
    "# Create the training job in Amazon SageMaker\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "response = sagemaker.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    HyperParameters={\n",
    "        'early_stopping_rounds': '5',\n",
    "        'num_round': '10',\n",
    "        'objective': 'binary:logistic', ## Binary classification since readmission will be Yes or NO. Get probability of binary classification\n",
    "        'eval_metric': 'auc' ## Evaluation Metic is Area Under Curve\n",
    "\n",
    "    },\n",
    "    AlgorithmSpecification={\n",
    "        'TrainingImage': training_image,\n",
    "        'TrainingInputMode': 'File',\n",
    "    },\n",
    "    RoleArn=role,\n",
    "    InputDataConfig=[\n",
    "        {\n",
    "            'ChannelName': 'train',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}'.format(bucket+'/'+train_prefix),\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'text/csv',\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'None',\n",
    "            'InputMode': 'File'\n",
    "        },\n",
    "        {\n",
    "            'ChannelName': 'validation',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}'.format(bucket+'/'+validation_prefix),\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'text/csv',\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'None',\n",
    "            'InputMode': 'File'\n",
    "        },\n",
    "    ],\n",
    "    OutputDataConfig={\n",
    "        'S3OutputPath': 's3://{}/xgb'.format(bucket),\n",
    "        'KmsKeyId' : sse_kms_id\n",
    "    },\n",
    "    ResourceConfig={\n",
    "        'InstanceType': 'ml.m5.4xlarge', ## For XGBoost use memory optimized instances since all the data is loaded into memory so we need memory intensive Ec2 instance\n",
    "        'InstanceCount': 2, ## Distributed training\n",
    "        'VolumeSizeInGB': 10\n",
    "    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 3600\n",
    "    },)\n",
    "\n",
    "print('{}\\n'.format(response))\n",
    "\n",
    "# Monitor the status until completed\n",
    "job_run_status = sagemaker.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']\n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    job_run_status = sagemaker.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']\n",
    "    print (job_run_status)\n",
    "    time.sleep(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning to find the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore Hyperparameters of SageMaker XGBoost Algorithm at https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Amazon SageMaker XGBoost Training Job\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Get XGBoost container image for current region\n",
    "training_image = get_image_uri(region, 'xgboost', repo_version=\"0.90-1\")\n",
    "\n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"TrainingImage\": training_image,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "      {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}'.format(bucket+'/'+train_prefix)\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"ChannelName\": \"validation\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}'.format(bucket+'/'+validation_prefix)\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": \"s3://{}/xgb\".format(bucket),\n",
    "      \"KmsKeyId\" : sse_kms_id\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "      \"InstanceCount\": 2, ## Distributed training\n",
    "      \"InstanceType\": \"ml.m5.4xlarge\",\n",
    "      \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "      \"eval_metric\": \"auc\",\n",
    "      \"num_round\": \"100\",\n",
    "      \"objective\": \"binary:logistic\",\n",
    "      \"rate_drop\": \"0.3\",\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 3600\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"eta\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"min_child_weight\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"2\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"alpha\",            \n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"max_depth\",\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 5,\n",
    "      \"MaxParallelTrainingJobs\": 2\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"validation:auc\",\n",
    "      \"Type\": \"Maximize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique training job name\n",
    "import random\n",
    "import string\n",
    "\n",
    "tuning_job_name = 'xgboost-readmission-'+''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(8))\n",
    "\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                            HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                            TrainingJobDefinition = training_job_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the status until completed\n",
    "job_run_status = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)['HyperParameterTuningJobStatus']\n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    job_run_status = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)['HyperParameterTuningJobStatus']\n",
    "    print (job_run_status)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying an Amazon SageMaker Endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a set of model artifacts, set up an inference pipeline that executes sequentially in Amazon SageMaker. You start by setting up a Model, which will point to all of your model artifacts, then you setup an **SageMaker Endpoint configuration** to specify your instance configuration, and finally you stand up an **SageMaker Endpoint**. \n",
    "\n",
    "With this endpoint, you pass the raw data and no longer need to write pre-processing logic in your application code. The same pre-processing steps that ran for training can be applied to inference input data for better consistency and ease of management.\n",
    "\n",
    "**Deploying a model in SageMaker requires two components:**\n",
    "- Docker image residing in ECR.\n",
    "- Model artifacts residing in S3.\n",
    "\n",
    "**SparkML**\n",
    "\n",
    "For SparkML, Docker image for MLeap based SparkML serving is provided by SageMaker team. For more information on this, please see SageMaker SparkML Serving. MLeap serialized SparkML model was uploaded to S3 as part of the SparkML job you executed in AWS Glue.\n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "For XGBoost, you use the same Docker image you used for training. The model artifacts for XGBoost are uploaded as part of the training job you just ran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Inference Pipeline consisting of SparkML & XGBoost models for a realtime inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get the best training job name \n",
    "best_training_job = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)['BestTrainingJob']['TrainingJobName']\n",
    "print ('Best training job : ' + best_training_job)\n",
    "\n",
    "info = smclient.describe_training_job(TrainingJobName=best_training_job)\n",
    "best_model_data_loc = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('Model Artifact Location : ' + best_model_data_loc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the schema of the payload via environment variable\n",
    "**SparkML serving container** needs to know the schema of the request that'll be passed to it while calling the **predict** method. In order to alleviate the pain of not having to pass the schema with every request, **sagemaker-sparkml-serving** allows you to pass it via an environment variable while creating the model definitions. *This schema definition will be required in your next step for creating a model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "schema = {\"input\":[{\"type\":\"string\",\"name\":\"encounters_encounterclass\"},{\"type\":\"string\",\"name\":\"patient_gender\"},{\"type\":\"string\",\"name\":\"patient_marital\"},{\"type\":\"string\",\"name\":\"patient_ethnicity\"},{\"type\":\"string\",\"name\":\"patient_race\"},{\"type\":\"string\",\"name\":\"encounters_reasoncode\"},{\"type\":\"string\",\"name\":\"encounters_code\"},{\"type\":\"string\",\"name\":\"procedures_code\"},{\"type\":\"double\",\"name\":\"patient_healthcare_expenses\"},{\"type\":\"double\",\"name\":\"patient_healthcare_coverage\"},{\"type\":\"double\",\"name\":\"encounters_total_claim_cost\"},{\"type\":\"double\",\"name\":\"encounters_payer_coverage\"},{\"type\":\"double\",\"name\":\"encounters_base_encounter_cost\"},{\"type\":\"double\",\"name\":\"procedures_base_cost\"},{\"type\":\"long\",\"name\":\"providers_utilization\"},{\"type\":\"double\",\"name\":\"age\"}],\"output\":{\"type\":\"double\",\"name\":\"features\",\"struct\":\"vector\"}}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a `PipelineModel` which comprises of the SparkML and XGBoost model in the right order\n",
    "\n",
    "Next you create a **SageMaker PipelineModel** with SparkML and XGBoost.The `PipelineModel` will ensure that both the containers get deployed behind a *single API endpoint* in the correct order. The same model would later be used for Batch Transform as well to ensure that a single job is sufficient to do prediction against the Pipeline. \n",
    "\n",
    "Here, during the `Model` creation for SparkML, you will pass the schema definition that you built in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls 's3://'$bucket'/spark-ml-model' --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update S3 prefix\n",
    "sparkml_model_prefix = 'spark-ml-model/2020/4/9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Get XGBoost container image for current region\n",
    "training_image = get_image_uri(region, 'xgboost', repo_version=\"0.90-1\")\n",
    "\n",
    "\n",
    "sparkml_data = 's3://{}/{}/{}'.format(bucket,sparkml_model_prefix,'model.tar.gz')\n",
    "# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\n",
    "xgb_model_data = '{}'.format(best_model_data_loc)\n",
    "xgb_model = Model(model_data=xgb_model_data, image=training_image)\n",
    "\n",
    "model_name = 'inference-pipeline-readmission-' + timestamp_prefix\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the `PipelineModel` to an endpoint for realtime inference\n",
    "Next you deploy the model you just created with the `deploy()` method to start an inference endpoint and then you will send some requests to the endpoint to verify that it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'inference-pipeline-readmission-ep-' + timestamp_prefix\n",
    "## Deploying two instaces for High Availability, SageMaker will automatically deploy them in different AZs\n",
    "sm_model.deploy(initial_instance_count=2, instance_type='ml.m5.4xlarge', endpoint_name=endpoint_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the status until completed\n",
    "endpoint_status = sagemaker.describe_endpoint(EndpointName='inference-pipeline-readmission-ep-' + timestamp_prefix)['EndpointStatus']\n",
    "while endpoint_status not in ('OutOfService','InService','Failed'):\n",
    "    endpoint_status = sagemaker.describe_endpoint(EndpointName='pipeline-xgboost-readmission')['EndpointStatus']\n",
    "    print(endpoint_status)\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the newly created inference endpoint with a payload to transform the data\n",
    "Now you invoke the endpoint with a valid payload that SageMaker SparkML Serving can recognize. Pass the input payload to the request:\n",
    "\n",
    "* Pass it as a valid CSV string. In this case, the schema passed via the environment variable will be used to determine the schema. For CSV format, every column in the input has to be a basic datatype (e.g. int, double, string) and it can not be a Spark `Array` or `Vector`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Test Data from S3\n",
    "\n",
    "You will first get the test data to get the prediction values. Use these values in the request to get the prediction. You will then see how the payload can be passed to the endpoint in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install PyArrow libraries to read test data set parquet files directly from S3 filesystem\n",
    "!pip install pyarrow==0.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Test CSV into pandas\n",
    "import pandas as pd \n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "# Python 3.6 or later\n",
    "p_dataset = pq.ParquetDataset(\n",
    "    f\"s3://{bucket}/{test_data_prefix}\",\n",
    "    filesystem=fs\n",
    ")\n",
    "\n",
    "test_data = p_dataset.read().to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch Test Data where the patient was not readmitted within 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_0 = test_data[(test_data['readmission'] == 0) & (test_data['encounters_reasoncode'] != 0) & (test_data['procedures_code'] != 0)]\n",
    "test_data_0.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch Test Data where the patient was readmitted within 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = test_data[(test_data['readmission'] == 1) & (test_data['encounters_reasoncode'] != 0)]\n",
    "test_data_1.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing the payload in CSV format\n",
    "Based on the test data, you can update the request provided to prediction API and validate the results. Note that you have used `binary:logistic` for training your model and the output will **probability** of patient being readmitted within 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "from sagemaker import Session as Sess\n",
    "# SageMaker session\n",
    "sess = Sess()\n",
    "\n",
    "## Payload schema = encounters_encounterclass,patient_gender,patient_marital,patient_ethnicity,patient_race,\n",
    "## encounters_reasoncode,encounters_code,procedures_code,patient_healthcare_expenses,\n",
    "## patient_healthcare_coverage,encounters_total_claim_cost,encounters_payer_coverage,encounters_base_encounter_cost,\n",
    "## procedures_base_cost,providers_utilization,age\n",
    "encounters_encounterclass='ambulatory'\n",
    "patient_gender='M'\n",
    "patient_marital='NM'\n",
    "patient_ethnicity='nonhispanic'\n",
    "patient_race='white'\n",
    "encounters_reasoncode='10509002'\n",
    "encounters_code='185345009'\n",
    "procedures_code='23426006'\n",
    "patient_healthcare_expenses='167365.25'\n",
    "patient_healthcare_coverage='2638.16'\n",
    "encounters_total_claim_cost='129.16'\n",
    "encounters_payer_coverage='49.16'\n",
    "encounters_base_encounter_cost='129.16'\n",
    "procedures_base_cost='516.65'\n",
    "providers_utilization='10924'\n",
    "age='8'\n",
    "\n",
    "payload = encounters_encounterclass+\",\"+patient_gender+\",\"+patient_marital+\",\"+patient_ethnicity+\",\"+patient_race+\",\"+encounters_reasoncode+\",\"+encounters_code+\",\"+procedures_code+\",\"+patient_healthcare_expenses+\",\"+patient_healthcare_coverage+\",\"+encounters_total_claim_cost+\",\"+encounters_payer_coverage+\",\"+encounters_base_encounter_cost+\",\"+procedures_base_cost+\",\"+providers_utilization+\",\"+age\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=csv_serializer,\n",
    "                                content_type=CONTENT_TYPE_CSV, accept=CONTENT_TYPE_CSV)\n",
    "print('probability of readmission:')\n",
    "print(predictor.predict(payload))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different payload\n",
    "Now let's update the request with different values for encounter_class, procedure_code, encounter_code, gender, patient_healthcare_expenses, etc. and see the results. Try to change values and check the prediction to understand the model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_encounterclass='ambulatory'\n",
    "patient_gender='F'\n",
    "patient_marital='M'\n",
    "patient_ethnicity='nonhispanic'\n",
    "patient_race='white'\n",
    "encounters_reasoncode='10509002'\n",
    "encounters_code='439740005'\n",
    "procedures_code='274804006'\n",
    "patient_healthcare_expenses='809476.7'\n",
    "patient_healthcare_coverage='39888.39'\n",
    "encounters_total_claim_cost='1456'\n",
    "encounters_payer_coverage='64.16'\n",
    "encounters_base_encounter_cost='129.16'\n",
    "procedures_base_cost='516.65'\n",
    "providers_utilization='12885'\n",
    "age='33'\n",
    "\n",
    "payload = encounters_encounterclass+\",\"+patient_gender+\",\"+patient_marital+\",\"+patient_ethnicity+\",\"+patient_race+\",\"+encounters_reasoncode+\",\"+encounters_code+\",\"+procedures_code+\",\"+patient_healthcare_expenses+\",\"+patient_healthcare_coverage+\",\"+encounters_total_claim_cost+\",\"+encounters_payer_coverage+\",\"+encounters_base_encounter_cost+\",\"+procedures_base_cost+\",\"+providers_utilization+\",\"+age\n",
    "\n",
    "print('probability of readmission:')\n",
    "print(predictor.predict(payload))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Deleting the Endpoint\n",
    "If you do not plan to use this endpoint, then it is a good practice to delete the endpoint so that you do not incur the cost of running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = sess.boto_session\n",
    "sm_client = boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
